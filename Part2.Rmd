---
title: "Part 2"
author: "Kahvi Patel"
output: html_document
---

ADL: (X1) total activities of daily living (low scores indicate that the elderly perform activities independently).

MEM: (X2) memory and behavioral problems (higher scores indicate more problems).

COG: (X3) cognitive impairment (lower scores indicate a greater degree of cognitive impairment).

## Data Import and Cleaning
```{r}
library(scales)

#read in .csv file
df <- read.csv('Data/stat123_regression.csv')  
```

### 1. After reading the data into R using the read.csv function, provide summary of the data. Comment on your results and especially on any unusual features in the data.

The variables are spread in different ways. The KBI has values from 28 to 115 while the COG has values from 0 to 27. Therefore, it seems like none of the variable were scaled in the same way, or else they would have similar maxes and mins.

```{r}
summary(df)
```
### 2. Produce the boxplot of ADL. Comment. Are there any outliers?

No, It doesn't look like there are any outliers. In fact, the boplot looks normally spread and not extremely skewed.

```{r}
boxplot(df$ADL)
```

### 3. Produce the scatterplots of Y and the X’s. Comment. Is a linear model appropriate for this data? Why or why not? Are the X’s correlated amongst themselves.

A linear model may be appropriate for the regression variables ADL, MEM and COG. The relationship would be scattered for all three variables since the data doesn't look explicitely grouped together. The relation for ADL and MEM would be positive.

```{r}
plot(df$ADL, df$KBI,
     main='KBI vs ADL',
     xlab="total activities of daily living", ylab="Korean Burden Inventory")

plot(df$MEM, df$KBI,
     main='KBI vs MEM',
     xlab="memory and behavioral problems", ylab="Korean Burden Inventory")

plot(df$COG, df$KBI,
     main='KBI vs COG',
     xlab="cognitive impairment", ylab="Korean Burden Inventory")
```

### 4. Fit univariable linear models, Y versus Xi for each of the three X regressor variables. 

```{r}
lmADL = lm(KBI~ADL, data = df)
lmMEM = lm(KBI~MEM, data = df)
lmCOG = lm(KBI~COG, data = df)
```

a. What are the estimated regression models?
```{r}
summary(lmADL)
summary(lmMEM)
summary(lmCOG)
```
b. Compare the univariable models above.
```{r}
plot(df$ADL, df$KBI,
     main='KBI vs ADL',
     xlab="total activities of daily living", ylab="Korean Burden Inventory")
abline(lmADL, col="blue")

plot(df$MEM, df$KBI,
     main='KBI vs MEM',
     xlab="memory and behavioral problems", ylab="Korean Burden Inventory")
abline(lmMEM, col="blue")

plot(df$COG, df$KBI,
     main='KBI vs COG',
     xlab="cognitive impairment", ylab="Korean Burden Inventory")
abline(lmCOG, col="blue")
```

c. Check the fit of the models and comment.

The MEM linear model fits the best, as shown below by its r-squared value. That said, none of the regression models have an value over 50% which indicates that the data is scattered.

```{r}
print(paste("ADL r-squared: ", percent(summary(lmADL)$r.squared)))
print(paste("MEM r-squared: ", percent(summary(lmMEM)$r.squared)))
print(paste("COG r-quared: ", percent(summary(lmCOG)$r.squared)))
```
d. Explain each of the estimated regression parameters (except the intercept) in words.

Coefficients:
- The slope coeficient is the value in the second row of the Estimates column. This value represents how much the Y value (KBI) increases for one step in the X value or regression variable.

Residual standard error:
- The residual standard error is the amount 

Multiple R-squared:
- The r-squared value represents the measure of the linear relationship between our regression variable and our response variable.

F-statistic:
- The F-test lets us quantify how well our data fits a model. The F-statistic = Sum of squares for regression/Sums of squares for error. 
- The p-value indicates how likely our null hypothesis (in this case a slope of zero) is. The scientific consensus is that if our p-value is < 0.05, then the result is statistically significant and the null hypothesis is rejected.